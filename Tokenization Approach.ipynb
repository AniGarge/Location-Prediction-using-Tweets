{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re     # Regular Expression library\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk   # Natural Language Processing library\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the basic natural language processes of removal of punctuations, tokenization,\n",
    "# removal of stop words and stemming.\n",
    "\n",
    "def tokenize(data):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    temp = data\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    temp = regex.sub(' ', temp)\n",
    "    temp = \"\".join(b for b in temp if ord(b) < 128)\n",
    "    words = nltk.word_tokenize(temp)\n",
    "    no_stop_words = [w.lower() for w in words if not w in stop_words]\n",
    "    stemmed = [ps.stem(item) for item in no_stop_words]\n",
    "\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses regular expressions to map the locations which contain any location.\n",
    "def location(data):\n",
    "    temp = data[(data.location.str.contains(r'[.]+ WA$'))\n",
    "            | (data.location.str.contains(r'[.]+ MA$'))\n",
    "            | (data.location.str.contains('Boston'))\n",
    "            | (data.location.str.contains('Seattle'))\n",
    "            | (data.location.str.contains(r'[.]+ Washington\\s'))\n",
    "            | (data.location.str.contains('Massachusetts'))]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the variable 1 for any tweet having Seattle or Washington in tweet otherwise 0.\n",
    "def maps(data):\n",
    "    targets = []\n",
    "    for location in data.location.apply(lambda x: x.encode('utf-8').strip()):\n",
    "        if (b'[.]+ WA$' in location) or (b'Seattle' in location) or (b'[.]+ Washington\\s' in location):\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to balance the dataset as if one class has more examples then the model predicts everything to be of majority class.\n",
    "def balance(data, targets):\n",
    "    new_data = data.copy()\n",
    "    if (len(targets[targets==1])) > (len(targets[targets==0])):\n",
    "        points_needed = len(targets[targets==1]) - len(targets[targets==0])\n",
    "        indices = np.where(targets == 0)\n",
    "    else:\n",
    "        points_needed = len(targets[targets==0]) - len(targets[targets==1])\n",
    "        indices = np.where(targets == 1)\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    indices = np.resize(indices, points_needed)\n",
    "    new_data = new_data.append(data.iloc[indices])\n",
    "    targets_to_add = targets[indices]\n",
    "    new_targets = np.concatenate([targets, targets_to_add])\n",
    "    return new_data, new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tweets_#superbowl.txt'\n",
    "\n",
    "# Collect tweets from superbowl\n",
    "tweets_ = []\n",
    "with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "    for row in f:\n",
    "        jrow = json.loads(row)\n",
    "        d = {\n",
    "            'tweet': jrow['title'],\n",
    "            'location': jrow['tweet']['user']['location']\n",
    "        }\n",
    "        tweets_.append(d)\n",
    "all_data = pd.DataFrame(tweets_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = location(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = maps(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, train_targets = balance(reduced_data, all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'thu', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Use the counter vectorizer and tfidf to transform the training data.\n",
    "vectorizer = CountVectorizer(analyzer='word', stop_words='english', tokenizer=tokenize)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "model = vectorizer.fit(data.tweet)\n",
    "train_counts=model.transform(data.tweet)\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use truncated SVD for dimensionality reduction as it deals better with sparse matrix.\n",
    "svd = TruncatedSVD(n_components=60, random_state=42)\n",
    "svd_model = svd.fit(train_tfidf)\n",
    "train_reduced=svd_model.transform(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to remove any irregular scales for the features. We used min max scaler in this case because every other \n",
    "# scalar was introducing negative values which cannot be processed by a naive bayes model.\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_data = min_max_scaler.fit_transform(train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV-Accuracy of Multinomial Naive Bayes: 0.741293977340489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77      1702\n",
      "           1       0.80      0.64      0.71      1652\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      3354\n",
      "   macro avg       0.75      0.74      0.74      3354\n",
      "weighted avg       0.75      0.74      0.74      3354\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1441  261]\n",
      " [ 598 1054]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used 10 fold cross validation with multinomial Naive Bayes to predict the location from the tweet.\n",
    "\n",
    "n=10\n",
    "kf = KFold(n_splits=n, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy = 0\n",
    "for i, j in kf.split(train_data):\n",
    "    X_train, X_test = train_data[i], train_data[j]\n",
    "    y_train, y_test = train_targets[i], train_targets[j]\n",
    "\n",
    "    clf = MultinomialNB(alpha=0.1).fit(X_train, y_train)\n",
    "    bayes_pred = clf.predict(X_test)\n",
    "    bayes_accuracy = np.mean(bayes_pred == y_test)\n",
    "    accuracy += bayes_accuracy\n",
    "\n",
    "print (\"Average CV-Accuracy of Multinomial Naive Bayes: \" + str(accuracy/k))\n",
    "print((classification_report(y_test, bayes_pred)))\n",
    "print (\"Confusion Matrix: \\n\",confusion_matrix(y_test, bayes_pred))\n",
    "bayes_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV-Accuracy of Logistic Regression: 0.8133870005963029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84      1702\n",
      "           1       0.91      0.70      0.79      1652\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      3354\n",
      "   macro avg       0.84      0.82      0.82      3354\n",
      "weighted avg       0.84      0.82      0.82      3354\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1588  114]\n",
      " [ 492 1160]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used 10 fold cross validation with Logistic Regression and generate the confusion matrix.\n",
    "accuracy = 0\n",
    "for i, j in kf.split(train_data):\n",
    "    X_train, X_test = train_data[i], train_data[j]\n",
    "    y_train, y_test = train_targets[i], train_targets[j]\n",
    "\n",
    "    logit = sk.LogisticRegression(solver='lbfgs').fit(X_train, y_train)\n",
    "    # Only picking the predictions where probability is greater than 0.5\n",
    "    probabilities = logit.predict(X_test)\n",
    "    lr_pred = (probabilities > 0.5).astype(int)\n",
    "    lr_accuracy = np.mean(lr_pred == y_test)\n",
    "    accuracy += lr_accuracy\n",
    "\n",
    "print (\"Average CV-Accuracy of Logistic Regression: \" + str(accuracy/k))\n",
    "print((classification_report(y_test, lr_pred)))\n",
    "print (\"Confusion Matrix: \\n\",confusion_matrix(y_test, lr_pred))\n",
    "lr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV-Accuracy of Linear SVM: 0.8131484794275492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.84      1702\n",
      "           1       0.91      0.70      0.79      1652\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      3354\n",
      "   macro avg       0.84      0.82      0.82      3354\n",
      "weighted avg       0.84      0.82      0.82      3354\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1591  111]\n",
      " [ 496 1156]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used linear svm with 10 fold cross validation.\n",
    "\n",
    "accuracy = 0\n",
    "for i, j in kf.split(train_data):\n",
    "    X_train, X_test = train_data[i], train_data[j]\n",
    "    y_train, y_test = train_targets[i], train_targets[j]\n",
    "\n",
    "    linear_SVM = LinearSVC(dual=False, random_state=42).fit(X_train, y_train)\n",
    "    svm_pred = linear_SVM.predict(X_test)\n",
    "    svm_accuracy = np.mean(svm_pred == y_test)\n",
    "    accuracy += svm_accuracy\n",
    "\n",
    "print (\"Average CV-Accuracy of Linear SVM: \" + str(accuracy/k))\n",
    "print((classification_report(y_test, svm_pred)))\n",
    "print (\"Confusion Matrix: \\n\",confusion_matrix(y_test, svm_pred))\n",
    "svm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV-Accuracy of Linear SVM: 0.6479427549194992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88      1702\n",
      "           1       0.91      0.83      0.87      1652\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      3354\n",
      "   macro avg       0.88      0.87      0.87      3354\n",
      "weighted avg       0.88      0.87      0.87      3354\n",
      "\n",
      "Confusion Matrix: \n",
      " [[1564  138]\n",
      " [ 283 1369]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i, j in kf.split(train_data):\n",
    "    X_train, X_test = train_data[i], train_data[j]\n",
    "    y_train, y_test = train_targets[i], train_targets[j]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=50,random_state=42).fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_accuracy = np.mean(svm_pred == y_test)\n",
    "    accuracy += rf_accuracy\n",
    "\n",
    "print (\"Average CV-Accuracy of Random Forest Classifier: \" + str(accuracy/k))\n",
    "print((classification_report(y_test, rf_pred)))\n",
    "print (\"Confusion Matrix: \\n\",confusion_matrix(y_test, rf_pred))\n",
    "rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
